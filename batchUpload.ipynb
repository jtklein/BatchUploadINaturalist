{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batchUpload.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtklein/BatchUploadINaturalist/blob/master/batchUpload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4pV0HXPty80A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In their recent paper, Heberling and Isaac ([paper here](https://bsapubs.onlinelibrary.wiley.com/doi/full/10.1002/aps3.1193)) argued for augmenting herbarium collections by linking them to observations recorded on the community based biodiversity forum iNaturalist (https://www.inaturalist.org/). In particular, this would make available photos of the plant and of its habitat before collecting and pressing it; data that has even greater value when linked to a physical specimen and which might otherwise be unavailable to potential users. Doing so works well if you are already using the smartphone app of iNaturalist in the field, as you can create observations on the fly and even use these to export collections data for databasing separately later. If you're sold on the value of the approach, what then about all that data that you collected in years past which might just be sitting more or less unused on your hard drive? In general it is also possible to create observations on the homepage or through the app from past observations. However, if your backlog of observations is large this can quickly become a massive task.\n",
        "Luckily, iNaturalist also has an API that you can directly call for standard tasks such as creating or deleting an observation. Using the API also makes it extremely quick to perform these actions in batch for several hundreds of observations.\n",
        "\n",
        "The goal of this post is to give you some ideas for a workflow on how to create several new observation records on iNaturalist. To make it more accessible we will do so for a real-life backlog from a biologist. In order to show the workflow, we have chosen to setup a Google Colaboratory notebook. These relatively new Google product has some cool features helpfull for us in this case. For example, that it provides a standardized machine setup, lets you share your code directly with other users interactively, has easy integration with Google Drive, a good place to share the photos to be uploaded. In principle it is a python Jupyter notebook in the cloud with a dedicated machine running your code. For further information about Colaboratory notebooks check out the [documentation](https://colab.research.google.com/notebooks/welcome.ipynb). We have chosen this setup because it is extremely easy to share code for this blog post, be aware though that in most cases it makes sense to run these scripts locally on your machine with a python runtime. However, this would require at least some experience working with python.\n",
        "\n",
        "In the specific example that we will use here, the setup is the following. In other words the bare bone **recipe** we follow is:\n",
        "\n",
        "*   The biologist has all the data collected about the plant in a table. In your case, this can be an export of an online database or locally stored file. In the example here this is an Excel table. (**One table**)\n",
        "*   The **photos** of the observations are stored **in one Google Drive folder**.\n",
        "* The **photos** available are linked to the data in the table by a **unique identifier**. In the example here the name of each file incorporates the collector initials plus collection number. This is sufficient to link it to the database.\n",
        "*  The biologist already has **an iNaturalist account** and also **one iNaturalist project** the observations will be tied to. The iNaturalist account has to be at least a curator in this project in order to add observations to it.\n",
        "* Keep in mind that in order to use the same setup with a Colaboratory notebook with access to a Google Drive a **Google account** is required.\n",
        "\n",
        "The basic outline of the script is this: We will read in the data table. For each row (observation) in the table we will check if there are photos available on the Google Drive. For each observation with photos we will create an iNaturalist observation and put it into one project.\n",
        "\n",
        "Each block of code has to be executed in sequence."
      ]
    },
    {
      "metadata": {
        "id": "M9fxu2XPNoD8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is an example of how to setup the Google Drive folder with the photos to be uploaded.\n",
        "\n",
        "![Example of Google Drive folder setup](https://github.com/jtklein/BatchUploadINaturalist/blob/master/Example_GoogleDrive_folder.png?raw=true)\n",
        "\n",
        "Here is an example of how to setup the data table. You will be prompted to uploaded it to the Colab instance if you follow our code.\n",
        "\n",
        "![Example of data table 1](https://github.com/jtklein/BatchUploadINaturalist/blob/master/Example_Data_Table_1.png?raw=true)\n",
        "\n",
        "![Example of data table 2](https://github.com/jtklein/BatchUploadINaturalist/blob/master/Example_Data_Table_2.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "qvOBdnp3UycZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install external packages\n",
        "Now let’s begin with the first block of code. Here, we need to install some external packages into the Colaboratory notebook instance. These notebooks are actually offered by Google to provide a fast machine learning prototyping experience. For this reason, already quite a few packages that we will need are installed from the start. However, we will need additional packages for connecting to Google Drive and the iNaturalist API. (You might get an error for incompatible requirements, you can ignore it for this block)"
      ]
    },
    {
      "metadata": {
        "id": "PjfkZFLJXJtf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3b394002-d253-469a-8223-c205373f836f"
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "# Install external packages\n",
        "# Install pyinaturalist\n",
        "!pip install -q pyinaturalist\n",
        "# Install the PyDrive wrapper\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    17% |█████▋                          | 10kB 20.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 40kB 3.5MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▎   | 51kB 4.3MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement requests~=2.18.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 20.3MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rd8lZuom6MvS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import packages\n",
        "We import the required packages into the python runtime."
      ]
    },
    {
      "metadata": {
        "id": "f97Z4wDnXpLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This only needs to be done once per notebook.\n",
        "# Import libraries\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import auth\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://gsuitedevs.github.io/PyDrive/docs/build/html/index.html\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# https://github.com/inbo/pyinaturalist\n",
        "from pyinaturalist.rest_api import create_observations\n",
        "from pyinaturalist.rest_api import add_photo_to_observation\n",
        "from pyinaturalist.node_api import get_observations\n",
        "from pyinaturalist.rest_api import delete_observation # in case we need to delete an observation\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWNSywCq6gLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define functions\n",
        "The next three blocks of code we will not need right away. We are declaring functions here that we will use later. Later, when we iterate over all rows in the table to create observations, you will see that there is a lot to do for each row. In order to keep this block further down smaller and more readable I am defining some functions here. Having these functions dedicated to one task also makes the codebase more reusable. For instance if we would need to add an observation to a post in a different notebook we can just take the definition of the function for that from here.\n",
        "\n",
        "Traditionally, in python the definitions of functions is at the beginning of the script. For now you can just execute these functions definitions to continue with the flow of the script. Later you can come back to check what these functions are doing internally. Or you can familiarize yourself with what these functions are doing now. "
      ]
    },
    {
      "metadata": {
        "id": "W9BbM3IW75cl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first function that we define here has one purpose: To extract the data from one row of the input table (corresponding to one observation in our case) into a form that is accepted by the iNaturalist API to create an observation. We know that there are a lot of different database formats out there, and in your case you are likely to have a different structure of your data table. For that reason you likely will need to change this part a lot to accomodate your structure. Keep in mind that it is generally speaking more beneficial to change the script to accomodate your data then the other way around. In the specific example here, the table has been exported from a [BRAHMS](https://herbaria.plants.ox.ac.uk/bol) database and includes a number of fairly standard fields.\n",
        "\n",
        "So, make a copy of this block into your notebook and change it to what you need. You can keep ours as a reference.\n",
        "\n",
        "The way we are doing it here is: We extract the information we need from one row of the database table by indexing into the row with the columns' names. For some data we have added some additional checks, simply because not all of the rows in our example dataset have this particuar information, and we need to avoid including empty datafields when creating observations. In this case, all records from the Western Cape in South Africa were known to have been from the habitat type \"Fynbos\" and the positional accuracy was hardcoded to 100 m; this information could otherwise be obtained from the table if it is recorded. Lastly, we transform the data into a python dictionary and return it back. For creating an observation we will be using these input parameters with the pyinaturalist package, for further possible input parameters you can refer to this [reference](https://github.com/inbo/pyinaturalist#create-a-new-observation) or directly [here](https://www.inaturalist.org/pages/api+reference#post-observations)."
      ]
    },
    {
      "metadata": {
        "id": "s4TfXkEUrwcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_observation_params(row):\n",
        "    \"\"\"\n",
        "    Extracts the data to store in the observation from the dataframes table row\n",
        "    and transforms it into a dictionary structure\n",
        "    \"\"\"\n",
        "    # Extracting the information from the dataframe table row\n",
        "    colnum = row['colnum']\n",
        "    description = row['DESCRIPTION']\n",
        "    number = row['NUMBER']\n",
        "    genus = row['GENUS']\n",
        "    species = row['SP1']\n",
        "    intra = row['SP2']\n",
        "    collector = row['COLLECTOR']\n",
        "    additional_collector = row['ADDCOLL']\n",
        "    latitude = row['LATDEC']\n",
        "    longitude = row['LONGDEC']\n",
        "    # Here we hardcoded a positional accuracy of 100m for the coordinates of the\n",
        "    # observation because this informationwas not available when sampling in the\n",
        "    # field, in your case you might also include this from the reference table\n",
        "    positional_accuracy = 100 # meters\n",
        "    place = row['GAZETTEER2']\n",
        "    # The following line would take the date from the photo metadata:\n",
        "    # photo_taken_at = row['photoTakenAt']\n",
        "    # For scanned photos this may be better obtained from the data table\n",
        "    year = row['year']\n",
        "    month = row['COLLMM']\n",
        "    day = row['COLLDD']\n",
        "    photo_taken_at = \"{}-{}-{}\".format(year, month, day)\n",
        "    institution_code = row['DUPLICATES']\n",
        "    identified_by = row['DETBY']\n",
        "    date_identified = row['DETYEAR']\n",
        "    \n",
        "    # We derived the time when the observation was made from the photo's metadata\n",
        "    # in case there was none given, we extract this information from the table row\n",
        "    #  if pd.isnull(photo_taken_at):\n",
        "    #  photo_taken_at = \"{}-{}-{}\".format(year, month, day)\n",
        "    \n",
        "    # We build a taxon string to send as initial identification for the observation\n",
        "    # In this case here we extract it from different fields in the table row\n",
        "    # Note: the string does not contain intraspecific designator (e.g. ssp. or var.)\n",
        "    taxon = genus\n",
        "    # If species epithet was given\n",
        "    if not pd.isnull(species):\n",
        "      taxon = taxon + \" \" + species\n",
        "    # If intraspecific was given\n",
        "    if not pd.isnull(intra):\n",
        "      taxon = taxon + \" \" + intra\n",
        "    \n",
        "    # We build a comma-seperated-list of tags that we want to add to the observation\n",
        "    # In this case the unique identifier number, genus name, species epithet\n",
        "    tag_list = str(colnum)\n",
        "    if not pd.isnull(genus):\n",
        "      tag_list = tag_list + \", \" + str(genus)\n",
        "    if not pd.isnull(species):\n",
        "      tag_list = tag_list + \", \" + str(species)\n",
        "    \n",
        "    # Dictionary structure of the params we want to send along\n",
        "    params = {\n",
        "      'observation':\n",
        "        {\n",
        "          'species_guess': taxon,\n",
        "          'tag_list': tag_list,\n",
        "          'observed_on_string': photo_taken_at,\n",
        "          'latitude': latitude,\n",
        "          'longitude': longitude,\n",
        "          'positional_accuracy': positional_accuracy,\n",
        "          'place_guess': place,\n",
        "          'description': description,\n",
        "          'observation_field_values_attributes': [],\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Because not every row/observation in the original data table has values for all fields\n",
        "    # we are checking which fields of the resulting dictionary have empty values and remove these\n",
        "    emptyKeys = []\n",
        "    for key, value in params['observation'].items():\n",
        "      if key == 'observation_field_values_attributes':\n",
        "        continue\n",
        "      if pd.isnull(value):\n",
        "        emptyKeys.append(key)\n",
        "    for key in emptyKeys:\n",
        "      del params['observation'][key]\n",
        "    \n",
        "    # We are adding the values for additional fields if given\n",
        "    # recordedBySymbiota\n",
        "    if not pd.isnull(collector):\n",
        "      params['observation']['observation_field_values_attributes'].append(\n",
        "        {'observation_field_id': 8958,'value': collector}\n",
        "      )\n",
        "      \n",
        "    # recordNumberdwc\n",
        "    if not pd.isnull(number):\n",
        "      params['observation']['observation_field_values_attributes'].append(\n",
        "        {'observation_field_id': 8953,'value': number}\n",
        "      )\n",
        "      \n",
        "    # associatedCollectorsSymbiota\n",
        "    if not pd.isnull(additional_collector):\n",
        "      params['observation']['observation_field_values_attributes'].append(\n",
        "        {'observation_field_id': 8790,'value': additional_collector}\n",
        "      )\n",
        "      \n",
        "    # identifiedBydwc\n",
        "    if not pd.isnull(identified_by):\n",
        "      params['observation']['observation_field_values_attributes'].append(\n",
        "        {'observation_field_id': 9598,'value': identified_by}\n",
        "      )\n",
        "      \n",
        "    # institutionCodedwc\n",
        "    if not pd.isnull(institution_code):\n",
        "      params['observation']['observation_field_values_attributes'].append(\n",
        "        {'observation_field_id': 10040,'value': institution_code}\n",
        "      )\n",
        "    \n",
        "    return params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyZxt82q-pMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function that we define here has one purpose: To take one photo from our Google Drive and upload it to one already existing iNaturalist observation.\n",
        "\n",
        "The way we are doing it here is: We create a cursor to the file that we need. Which file we need is provided to the function by the file's id. Then we download this file into the notebooks runtime. By using the pyinaturalist packages's call to the iNaturalist API we upload the photo file to an already existing observation (provided by observation ID). This is an authenticated call to the API, so we need to provide our iNaturalist account API access token. After we are finished we clean up behind ourselves and delete the downloaded photo from the runtime."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pYi3C-fYvYiB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_and_upload_photo_to_observation(photo_id: str=\"\", observation_id: str=\"\", access_token: str=\"\"):\n",
        "    \"\"\"\n",
        "    Get a photo from Google Drive and upload it to an iNaturalist observation\n",
        "    \"\"\"\n",
        "    # Download the file into this Colaboratory notebook instance\n",
        "    downloaded = drive.CreateFile({'id': photo_id})\n",
        "    downloaded.GetContentFile(photo_id)\n",
        "    # Call to iNaturalist API to add this photo to an existing observation\n",
        "    r = add_photo_to_observation(\n",
        "      observation_id=observation_id,\n",
        "      file_object=open(photo_id, 'rb'),\n",
        "      access_token=access_token\n",
        "    )\n",
        "    # Remove the photo file from this Colaboratory instance no longer needed\n",
        "    os.remove(photo_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P8PxxjsPAqNP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function that we define here has one purpose: To add an existing iNaturalist observation to an existing iNaturalist project.\n",
        "\n",
        "The way we are doing it here is: We create a dictionary with the ID of the observation to be added. Then we call the iNaturalist API directly and post to it that we want to add this observation to the project with given ID. By the time of writing this blog post, the function is not implemented in the pyinaturalist package yet. This is an authenticated call to the API, so we need to provide our iNaturalist account API access token."
      ]
    },
    {
      "metadata": {
        "id": "Gl-aHTFJFoVO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_observation_to_project(observation_id: str=\"\", project_id: str=\"\", access_token: str=\"\"):\n",
        "    \"\"\"\n",
        "    Use the iNaturalist API to add an existing observation to an existing project\n",
        "    Reference for this API call:\n",
        "    https://api.inaturalist.org/v1/docs/#!/Projects/post_projects_id_add\n",
        "    \"\"\"\n",
        "    # The payload of the API call\n",
        "    payload = {\n",
        "      \"observation_id\": observation_id\n",
        "    }\n",
        "    # This call needs to be authenticated\n",
        "    headers = {\"Authorization\": \"Bearer %s\" % access_token}\n",
        "    # Call the API\n",
        "    response = requests.post(\n",
        "        \"https://api.inaturalist.org/v1/projects/{}/add\".format(project_id),\n",
        "        params=payload,\n",
        "        headers=headers\n",
        "    )\n",
        "    return response.json()\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xaaGkK_fL1MW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticate with Google Drive\n",
        "As first real step in our script we will establish a connection to our Google Drive. If everything goes right we will be shown a link. On the page following this link you will be asked for permission to connect your Drive to this notebook instance. Once granted you will receive a code that you need to paste in the form and press enter.\n",
        "\n",
        "The Drive should now be connected. In principle this needs to be done only once at the start of the notebook. However, infrequently I have encountered an error like this: \"InvalidConfigError: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)\". Then it helps to just let this code run once again.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bNGky3IqsdAg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbKs65c_Bl54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# List the available photos\n",
        "Next to authenticating ourselves with our Google Drive we will use the Google Drive API to list all files of a given folder. Remember, in our example all photos of observations are in one folder. What we are doing here in this block of code is: First, we use the Google Drive API to search for a folder. We can use the API to list us all files that match certain search criteria. For finding our folder we can search for all folder with the query term \"mimeType contains 'application/vnd.google-apps.folder'\",  and the one folder needed with a specific string for the title of the folder \"title = Erica\". For this part of the script in our example we rely on only one folder being found with this title. Please keep in mind that the search by title is a prefix pattern matching, so if you have multiple files starting with this search term you will get multiple search results. Next, after we have identified the folder with the photos we again ask for a list of all files that are present in this folder. In our example we will only have photos in the folder. Make sure to include some more search criteria to exclude files that you do not need from this folder. We will reuse the list of photos in a later block.\n",
        "\n",
        "For a complete reference to the available search criteria for listing Google Drive files you can check the [documentation](https://developers.google.com/drive/v2/web/search-parameters)."
      ]
    },
    {
      "metadata": {
        "id": "I_vWHnpvsrZD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# We are using the Google Drive API to list the photos\n",
        "# We are looking for the folder that contains all the photos we will use\n",
        "# In this case we are looking for a folder called \"iNaturalist_Danth\",\n",
        "# result is a list of folders that match the search criteria\n",
        "folders = drive.ListFile({'q': \"mimeType contains 'application/vnd.google-apps.folder' and title contains 'iNaturalist_Erica'\"}).GetList()\n",
        "# Get the ID of the folder\n",
        "for folder in folders:\n",
        "  folderID = folder['id']\n",
        "  print('folder {}, id {}'.format(folder['title'], folder['id']))\n",
        "# Use the Google Drive API to list all files in the folder found by ID.\n",
        "listed = drive.ListFile({ 'q': \"'{}' in parents and trashed = false\".format(folderID) }).GetList()\n",
        "# Show some information about the files in the folder\n",
        "for file in listed:\n",
        "  print('file {}, id {}'.format(file['title'], file['id'])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lx-t4DYcpbZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare the data\n",
        "Next we will import our data about the observations into the notebook runtime. As mentioned earlier, in the example here the information about the observations is in the form of a table file. Specifically, we will use an Excel table file. In order for the information to be ready to use we need to upload this file to the Colaboratory instance. Here, after executing the code block the Google Colaboratory notebook provides us with a nice little upload dialog. Choose the data table file you want to use in the dialog. The file will be uploaded to the file system of the host of your runtime. You can inspect the files present in the runtime in the left-hand panel. As seen above in principal you can also host the database table on Google Drive and access it from there directly by following the above example. This is just to show you a different way of importing information into the runtime. If you run this example locally you need to locate your data file by your own.\n",
        "\n",
        "The uploaded file is being parsed and transformed into a pandas DataFrame. pandas is a prominent python library for data handling. If you are not familiar with pandas don’t sweat it, we have tried to keep the script as easy as possible. At the end of the block we can inspect the first few rows of the uploaded table."
      ]
    },
    {
      "metadata": {
        "id": "nzgBiWsLbKcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prompt an upload dialog\n",
        "uploaded = files.upload()\n",
        "# Get the filename of the uploaded file\n",
        "filename = list(uploaded.keys())[0]\n",
        "# Transform the table into a pandas dataframe\n",
        "df = pd.read_excel(filename, engine='xlrd')\n",
        "# Inspect the result\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BICMrcV-YRej",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order not to change the orginal data table that we first imported, we will create a copy to work with. We already made a copy by uploading it from outside the runtime, however, it is also good to keep an unchanged table present in your runtime in case we need to access it again.\n",
        "In the example here we are also only using a subset of the originally imported table (only the subset where Pirie, MD is the collector). In our case this comes from the fact that not all observations in this example dataset are made by the same person, so we restrict ourselves in using only the data of one person to be uploaded. The other collectors of data can use the same script to upload their data but need to change the selection criteria here. Carefully we inspect the new table structure."
      ]
    },
    {
      "metadata": {
        "id": "DW76UtCpWf2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will only use those rows of the table for which the collector is \"Pirie, M.D.\"\n",
        "# we create a copy to not alter the original\n",
        "mask = df['COLLECTOR'] == 'Pirie, M.D.'\n",
        "pirieList = df.copy()[mask]\n",
        "pirieList.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1-tYuyAbYL86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the rows in the table (observations) we will check if there are photos available that can be uploaded. You can in principle also create iNaturalist observations without photos, however, in our example here we restricted ourselves to only those with photos available.\n",
        "To do so, we iterate over each row in the table and check if in the list of files we created earlier there are files present that match a unique identifier present in the dataset. In our example we are using a combination of collectors initials and a collection number. In case we have a match with this unique identifier we extract the file’s ID and also a time stamp for when the photo was taken from the photo’s metadata. We append the information to the DataFrame."
      ]
    },
    {
      "metadata": {
        "id": "22ZEOfBzoSSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Iterate over the rows of the table (i.e. observations)\n",
        "for index, row in pirieList.iterrows():\n",
        "  # Extract the unique identifier (collector+number) for this row\n",
        "  colnum = \"MP{}\".format(row['NUMBER'])\n",
        "  query = colnum + \"_\"\n",
        "  photos = { 'photos': [] }\n",
        "  # Iterate over the list of photos in the Google Drive folder\n",
        "  for file in listed:\n",
        "    # If the unique identifier is in the file title\n",
        "    if query in file['title']:\n",
        "      # Add the file id and metadata about when the photo was taken to the table\n",
        "      pirieList.loc[index, 'colnum'] = colnum\n",
        "      photos['photos'].append(file['id'])\n",
        "      if 'imageMediaMetadata' in file:\n",
        "        if 'date' in file['imageMediaMetadata']:\n",
        "          # Date of the last picture\n",
        "          pirieList.loc[index, 'photoTakenAt'] = file['imageMediaMetadata']['date']\n",
        "  if len(photos['photos']) > 0:\n",
        "    pirieList.loc[index, 'photos'] = photos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adZCARKeXkYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticate with iNaturalist\n",
        "When we want to perform any actions on iNaturalist via the API that are tied to a user account we need to authenticate ourselves to their service. Otherwise their server has no way of knowing who is uploading these observations. Tasks that require authentication are for example creating or deleting observations among many others. For an exhaustive list of tasks that can require authentication you should check the API [documentation](https://api.inaturalist.org/v1/docs/). Several ways of authenticating yourselves to the API are available. For example, you can use a username plus password combination like in the homepage of iNaturalist. However, using such private information is not recommended when we share the notebook document. An other way of authenticating is by an access token that iNaturalist hands out to you if you are already logged into the website version. So, after you have logged into the website version go to this link here and you will see your token: [https://www.inaturalist.org/users/api_token](https://www.inaturalist.org/users/api_token). Please paste the entire information you see on the screen into this field here. This token is always only viable for 12 hours a piece, so if you come back to this notebook at a later point make sure to replace this token. The value to paste in here should look like this:  \n",
        "\n",
        "\n",
        "```\n",
        "{\"api_token\":\"eyJhbGciOiJIUzUxMiJ9.eyJ1c2VyX2lkIjo1MzA2NTksImV4cCI6MTU0ODUwMDY0NH0.SKKQsSMSofOhAwkPnFJ_m0tTCu1iFyVBhgtltjLeW49VtQwk_n2rQ3OtlkHTxHccfTyNg9WwZdhAV_yxQWVimg\"}\n",
        "\n",
        "```\n",
        "\n",
        "For the interested, you can double click into the form field anywhere to see how the code for this looks like."
      ]
    },
    {
      "metadata": {
        "id": "IpT0fEvMkYvE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Paste your entire iNaturalist api_token object here { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "# Get the value from the input form\n",
        "api_token =  #@param {type:\"raw\"}\n",
        "# Validate the input and use it if OK\n",
        "token = None\n",
        "if type(api_token) == dict:\n",
        "  try:\n",
        "    token = api_token['api_token']\n",
        "    print(token)\n",
        "  except (KeyError):\n",
        "    print(\"Your token object must have the key \\\"api_token\\\"\")\n",
        "else:\n",
        "  print(\"Your token does not have the correct format.\\nIt needs to be a python dictionary.\\nIt should look something like this:\\n {\\\"api_token\\\": \\\"xyz123\\\"}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzQi5yP4XjMP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we create new observations on iNaturalist we will have some checks to see if there might be already the same observation present. For this reason we need the iNaturalist username.\n",
        "\n",
        "For the interested, you can double click into the form field anywhere to see how the code for this looks like."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bhN4BfAAIWiZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Paste your iNaturalist username here { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "# Get the value from the input form\n",
        "username = \"\" #@param {type:\"string\"}\n",
        "# Validate the input and use it if OK\n",
        "user_id = None\n",
        "if type(username) == str:\n",
        "  user_id = username\n",
        "  print(user_id)\n",
        "else:\n",
        "  print(\"Your username does not have the correct format.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yp3I8y7jXap_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create new observations in batch\n",
        "Now, finally, the most important part. In this block of code we will perform all the main actions of this script. We will create an observation, add its photos and add this new observation to a project.\n",
        "In the outer loop you can see that we are iterating over all rows of the table (DataFrame). Each row consists of the data of one observation. Each observation is found in only one row. In the example here we only want to create a new iNaturalist observation if we have some photos for this observation. So, we skip all observations that do not have photos. Then, with the unique identifier for each observation (collector initials plus collection number) we check if there is already an observation present with the same identifier on iNaturalist. We are here searching in the list of tags of the observations. The search query params can be changed of course. For a reference to searching see the iNaturalist API [documentation](https://api.inaturalist.org/v1/docs/#!/Observations/get_observations).\n",
        "\n",
        "For creating an observation we are extracting all the necessary information from the row by using a function we defined earlier in our script. For details on this function check above. Then, we create a new observation via the iNaturalist API by using the pyinaturalist package. We are adding the new observation's ID to our row, to be saved for later. For each of the photos associated with this observation we will now use the function to upload them to iNaturalist, as described above. Lastly, if all was successful we add the new observation to the already existing project that they should be housed in. To add the observation we are using the function defined above. Keep in mind that you need to have at least curator rights in this already existing project to add observations. Also, we are using the project ID here directly. You need to paste it from iNaturalist for your project."
      ]
    },
    {
      "metadata": {
        "id": "MbOLPgLQnEqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For each row in the dataframe\n",
        "for index, row in pirieList.iterrows():\n",
        "  # If there are photos found\n",
        "  if not pd.isnull(row['photos']):\n",
        "    # The supposedly unique identifier\n",
        "    colnum = row['colnum']\n",
        "    # Check if there is already an observation with this collector+number\n",
        "    observations = get_observations(params={ 'q': colnum, 'search_on': 'tags', 'user_id': user_id })\n",
        "    if observations['total_results'] > 0:\n",
        "      print('There is already an observation by {} with the number {} in tags'.format(user_id, colnum))\n",
        "      continue\n",
        "    # Prepare the paramaters for creating an observation with the iNaturalist API\n",
        "    params = make_observation_params(row)\n",
        "    print(params)\n",
        "    # continue\n",
        "    # API call to create the observation\n",
        "    r = create_observations(params=params, access_token=token)\n",
        "    new_observation_id = r[0]['id']\n",
        "    print(\"Created new iNaturalist observation, id: {}\".format(new_observation_id))\n",
        "    # Save the observation id to the dataframe\n",
        "    pirieList.loc[index, 'iNaturalist_ID'] = str(new_observation_id)\n",
        "    # Add each of the photos found to the observation\n",
        "    for photo in row['photos']['photos']:\n",
        "      print(\"Uploading photo: {}\".format(photo))\n",
        "      get_and_upload_photo_to_observation(photo_id=photo, observation_id=new_observation_id, access_token=token)\n",
        "    # Add the new observation to an existing project\n",
        "    # the project ID is hardcoded to one specific project\n",
        "    print(\"Adding observation to project\")\n",
        "    add_observation_to_project(observation_id=new_observation_id, project_id=35591, access_token=token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7Xo9J1QXY7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the new information\n",
        "In this block of code all we do is write the appended DataFrame to a .csv file. We added some temporary information to the DataFrame during the script which is no longer needed. We keep the column with the resulted iNaturalist IDs of the new observations only. After execution you can find the new file in the left-hand panel of the notebook. You can also download it from there."
      ]
    },
    {
      "metadata": {
        "id": "oizM8yTR9Q0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a deep copy of the dataframe\n",
        "output_df = pirieList.copy()\n",
        "# Remove columns only needed for within this script\n",
        "output_df.drop(columns='colnum', inplace=True)\n",
        "output_df.drop(columns='photoTakenAt', inplace=True)\n",
        "output_df.drop(columns='photos', inplace=True)\n",
        "# Save to .csv in the notebook instance\n",
        "output_df.to_csv('output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JL52BYwyN2B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Troubleshooting\n",
        "If something goes wrong you can delete all observations you created with this snippet."
      ]
    },
    {
      "metadata": {
        "id": "adGYRLd5b4nn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For all rows in the dataframe\n",
        "for index, row in pirieList.iterrows():\n",
        "  # If there is an iNaturalist id delete the observation\n",
        "  id = row['iNaturalist_ID']\n",
        "  if not pd.isnull(id):\n",
        "    try:\n",
        "      delete_observation(int(id), token)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}